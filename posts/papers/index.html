<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
   <link rel="stylesheet" href="/libs/highlight/github.min.css">
   
  <link rel="stylesheet" href="/css/franklin.css">
<link rel="stylesheet" href="/css/poole_hyde.css">
<!-- style adjustments -->
<style>
  html {font-size: 17px;}
  .franklin-content {position: relative; padding-left: 8%; padding-right: 5%; line-height: 1.35em;}
  @media (min-width: 940px) {
    .franklin-content {width: 100%; margin-left: auto; margin-right: auto;}
  }
  @media (max-width: 768px) {
    .franklin-content {padding-left: 6%; padding-right: 6%;}
  }
</style>
<link rel="icon" href="/assets/lichen.png">

   <title>Maths & ML Gems</title> 

</head>
<body>
<div class="sidebar">
  <div class="container sidebar-sticky">

    <div class="sidebar-about">

      <h1><a class="sidebar-about" href="/">Simon Coste</a></h1>
      <p class="sidebar-about">Mathematics</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item " href="/">About</a>
      <a class="sidebar-nav-item " href="/research/">Research</a>
      <a class="sidebar-nav-item " href="/teaching/">Teaching</a>
      <a class="sidebar-nav-item " href="/notes/">Notes</a>
      <a class="sidebar-nav-item " href="/talk/">Talks</a>
    </nav>

    <div class="sidebar-foot">
      <p>&copy; Simon Coste, modified: April 25, 2024. <br> Built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language. </a></p>
    </div>


  </div>
  

  
</div>
<div class="content container">

<!-- Content appended here -->


<div class="franklin-content">
   <h1 class="page-title"> Maths & ML Gems </h1> 
   <span class="page-date"> 2024 </span> 
</div>
<div class="franklin-content">
<p>This is a list of wonderful papers in machine learning, reflecting my own tastes and interests. </p>
<ul>
<li><p><a href="http://mlsp.cs.cmu.edu/courses/fall2010/class14/lloyd.pdf">Least Squares Quantization in PCM</a> by Stuart P Lloyd &#40;1982 but he got the method twenty years earlier&#41;. Definition of Lloyd&#39;s algorithm for k-means clustering. </p>
</li>
<li><p><a href="http://www.stat.yale.edu/~hz68/619/Stein-1961.pdf">The James-Stein paradox in estimation</a> by Jamest and Stein, 1961</p>
</li>
<li><p><a href="https://link.springer.com/article/10.1007/BF02291478">Generalized Procrustes analysis</a> by Gower &#40;1975&#41;</p>
</li>
<li><p><a href="https://link.springer.com/article/10.1007/BF02551274">Universal approximation theorem</a> by Cybenko &#40;1989&#41;</p>
</li>
<li><p><a href="https://arxiv.org/pdf/math/0409186.pdf">Compressed sensing paper</a> by Candès, Romberg, and Tao &#40;2006&#41;.</p>
</li>
<li><p><a href="https://proceedings.neurips.cc/paper_files/paper/1999/file/6a5dfac4be1502501489fc0f5a24b667-Paper.pdf">Scale Mixtures of Gaussians and the Stastistics of Natural Images</a> by Wainwright and Simoncelli &#40;1999&#41;.</p>
</li>
<li><p><a href="https://www.di.ens.fr/~fbach/courses/fall2010/Bishop_Tipping_1999_Probabilistic_PCA.pdf">Probabilistic PCA</a> by Tipping and Bishop &#40;1999&#41;. Lightweight generative model. </p>
</li>
<li><p><a href="https://link.springer.com/content/pdf/10.1023/A:1008923215028.pdf">Annealed importance sampling</a> by Radford Neal &#40;2001&#41;. </p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/document/4767851">A computational approach to edge detection</a> by John Canny &#40;1986&#41;</p>
</li>
<li><p><a href="https://hastie.su.domains/Papers/spc_jcgs.pdf">The Sparse PCA paper</a> by Zou, Hastie, Tibshirani &#40;2006&#41;.</p>
</li>
<li><p><a href="https://arxiv.org/abs/math/0403022">The BBP transition</a> by Baik, Ben Arous, and Péché &#40;2004&#41;. </p>
</li>
<li><p><a href="https://proceedings.neurips.cc/paper/2001/file/801272ee79cfde7fa5960571fee36b9b-Paper.pdf">On spectral clustering</a> by Ng, Jordan and Weiss &#40;2001&#41;.</p>
</li>
<li><p><a href="https://www.jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf">Hyvärinen&#39;s Score Matching paper</a> in 2005. </p>
</li>
<li><p><a href="https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">The AlexNet paper</a> by Krizhevsky, Sutskever, and Hinton &#40;2012&#41;.</p>
</li>
<li><p><a href="https://dl.acm.org/doi/pdf/10.1145/2184319.2184343">Exact Matrix Completion via Convex Optimization</a> by Candes and Recht &#40;2009&#41;. Matrix completion via optimization.</p>
</li>
<li><p><a href="https://arxiv.org/pdf/0901.3150.pdf">Matrix completion from a few entries</a> by Keshavan, Montanari and Oh &#40;2009&#41;. Matrix completion from SVD tresholding. </p>
</li>
<li><p><a href="https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf">Adaptive mixtures of experts</a> by Jacobs et al. Introduces the famous MoE method. </p>
</li>
<li><p><a href="https://proceedings.mlr.press/v37/rezende15.pdf">Normalizing flows</a> by Rezende and Mohamed &#40;2015&#41;.</p>
</li>
<li><p><a href="https://arxiv.org/abs/1412.6980">The ADAM optimizer</a> by Kingma and Ba &#40;2014&#41;.</p>
</li>
<li><p><a href="https://arxiv.org/pdf/1812.09902.pdf">Invariant and equivariant graph networks</a> by Maron et al. &#40;2019&#41;. They compute the dimension of invariant and equivariant linear layers and study GNN expressivity. </p>
</li>
<li><p><a href="https://arxiv.org/abs/1503.03585">The original paper introducing generative diffusion models</a>, by Sohl-Dickstein et al &#40;2015&#41;</p>
</li>
<li><p><a href="https://arxiv.org/abs/2011.13456">The second paper of diffusions</a> by Song et al &#40;2020&#41;</p>
</li>
<li><p><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf">The Stable Diffusion paper</a> by Rombach et al &#40;2021&#41;</p>
</li>
<li><p><a href="https://arxiv.org/abs/1806.07366">The Neural ODE paper</a> by Chen et al. &#40;2018&#41;</p>
</li>
<li><p><a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a>, 2017. This paper changed the world.</p>
</li>
<li><p><a href="https://arxiv.org/abs/2210.02747">Flow matching</a> by Lipman et al, 2022</p>
</li>
<li><p><a href="https://arxiv.org/abs/1806.07572">The NTK paper</a> by Jacot, Gabriel and Hongler &#40;2018&#41;.</p>
</li>
<li><p><a href="https://www.jmlr.org/papers/volume22/20-410/20-410.pdf">Implicit regularization in deep networks</a> by Martin and Mahonney &#40;2021&#41;. On the training dynamics of the hessian spectrum of DNNs. </p>
</li>
<li><p><a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf">Language models are few-shot learners</a> on LLM scaling laws</p>
</li>
<li><p><a href="https://arxiv.org/abs/2103.00065">Edge of Stability paper</a> by Cohen et al. </p>
</li>
<li><p><a href="https://arxiv.org/abs/2310.18988">A U-turn on double descent</a> by Curth et al. </p>
</li>
<li><p><a href="https://proceedings.mlr.press/v70/arjovsky17a/arjovsky17a.pdf">The Wasserstein GAN paper</a> by Arzovsky, Chintala and Bottou &#40;2017&#41;</p>
</li>
<li><p><a href="https://arxiv.org/pdf/1912.01412.pdf">Deep learning for symbolic mathematics</a> by Lample and Charton &#40;2019&#41;</p>
</li>
<li><p><a href="https://arxiv.org/abs/2201.09792">The Convmixer paper</a></p>
</li>
<li><p><a href="https://www.pnas.org/doi/full/10.1073/pnas.1802705116?doi&#61;10.1073/pnas.1802705116">Error in high-dimensional GLMs</a> by Barbier et al. &#40;2018&#41;</p>
</li>
<li><p><a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.108.188701">Spectral algorithms for clustering</a> by Nadakuditi and Newman, from an RMT perspective</p>
</li>
<li><p><a href="https://scholar.google.com/citations?view_op&#61;view_citation&amp;hl&#61;en&amp;user&#61;gkCjy_UAAAAJ&amp;citation_for_view&#61;gkCjy_UAAAAJ:O3NaXMp0MMsC">Spectral redemption in clustering sparse networks</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2010.11929">An image is worth 16x16 words</a>, the Vision Transformer paper by Dosovitskiy et al. &#40;2020&#41;</p>
</li>
</ul>
<!-- 
  Page footer is in the sidebar. 
 --></div><!-- CONTENT ENDS HERE -->
    </div>  <!-- div: content container -->
    
    
        


    
  </body>
</html>
