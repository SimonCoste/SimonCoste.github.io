<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
   <link rel="stylesheet" href="/libs/katex/katex.min.css">
     
   <link rel="stylesheet" href="/libs/highlight/github.min.css">
   
  <link rel="stylesheet" href="/css/franklin.css">
<link rel="stylesheet" href="/css/poole_hyde.css">
<!-- style adjustments -->
<style>
  html {font-size: 17px;}
  .franklin-content {position: relative; padding-left: 8%; padding-right: 5%; line-height: 1.35em;}
  @media (min-width: 940px) {
    .franklin-content {width: 100%; margin-left: auto; margin-right: auto;}
  }
  @media (max-width: 768px) {
    .franklin-content {padding-left: 6%; padding-right: 6%;}
  }
</style>
<link rel="icon" href="/assets/lichen.png">

   <title>Maths & ML Gems</title> 

</head>
<body>
<div class="sidebar">
  <div class="container sidebar-sticky">

    <div class="sidebar-about">

      <h1><a class="sidebar-about" href="/">Simon Coste</a></h1>
      <p class="sidebar-about">Mathematics</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item " href="/">About</a>
      <a class="sidebar-nav-item " href="/research/">Research</a>
      <a class="sidebar-nav-item " href="/teaching/">Teaching</a>
      <a class="sidebar-nav-item " href="/notes/">Notes</a>
      <a class="sidebar-nav-item " href="/talk/">Talks</a>
    </nav>

    <div class="sidebar-foot">
      <p>&copy; Simon Coste, modified: April 16, 2025. <br> Built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language. </a></p>
    </div>


  </div>
  

  
</div>
<div class="content container">

<!-- Content appended here -->


<div class="franklin-content">
   <h1 class="page-title"> Maths & ML Gems </h1> 
   <span class="page-date"> Updated in 2025 </span> 
</div>
<div class="franklin-content">
<p>This is a list of wonderful papers in machine learning, reflecting my own tastes and interests. </p>
<h2 id="numerical_maths"><a href="#numerical_maths" class="header-anchor">Numerical Maths</a></h2>
<ul>
<li><p><a href="https://acoustics.web.illinois.edu/pdfs/lele-1992.pdf">Compact finite difference schemes</a> by Lele &#40;1992&#41;. The paper that introduced the concept of compact finite difference schemes. </p>
</li>
<li><p><a href="https://link.springer.com/article/10.1007/BF02291478">Generalized Procrustes analysis</a> by Gower &#40;1975&#41;</p>
</li>
<li><p><a href="http://mlsp.cs.cmu.edu/courses/fall2010/class14/lloyd.pdf">Least Squares Quantization in PCM</a> by Stuart P Lloyd &#40;1982 but he got the method twenty years earlier&#41;. Definition of Lloyd&#39;s algorithm for k-means clustering. </p>
</li>
<li><p><a href="https://scispace.com/pdf/a-relationship-between-arbitrary-positive-matrices-and-eqco9zw27d.pdf">Sinkhorn’s algorithm</a> factorizes any matrix <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal">A</span></span></span></span> with positive entries into <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mn>1</mn></msub><mi>P</mi><msub><mi>D</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">D_1 P D_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> with <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">D_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> diagonal matrices and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span></span></span></span> bistochastic. It gave rise to the Sinkhorn algorithm in optimal transport. </p>
</li>
<li><p><a href="https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-22/issue-3/A-Stochastic-Approximation-Method/10.1214/aoms/1177729586.full">Robbins-Monro</a>, the original paper by Robbins and Monro &#40;1951&#41;. The first paper on stochastic optimization.</p>
</li>
</ul>
<h2 id="theoretical_statsmldl"><a href="#theoretical_statsmldl" class="header-anchor">Theoretical Stats/ML/DL</a></h2>
<ul>
<li><p><a href="http://www.stat.yale.edu/~hz68/619/Stein-1961.pdf">The James-Stein paradox in estimation</a> by Jamest and Stein, 1961. Sometimes, Maximum-likelihood is not the best estimator, even in a L2 world. </p>
</li>
<li><p><a href="https://proceedings.neurips.cc/paper_files/paper/1999/file/6a5dfac4be1502501489fc0f5a24b667-Paper.pdf">Scale Mixtures of Gaussians and the Stastistics of Natural Images</a> by Wainwright and Simoncelli &#40;1999&#41;.</p>
</li>
<li><p><a href="https://www.di.ens.fr/~fbach/courses/fall2010/Bishop_Tipping_1999_Probabilistic_PCA.pdf">Probabilistic PCA</a> by Tipping and Bishop &#40;1999&#41;. Lightweight generative model. </p>
</li>
<li><p><a href="https://link.springer.com/content/pdf/10.1023/A:1008923215028.pdf">Annealed importance sampling</a> by Radford Neal &#40;2001&#41;. </p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/document/4767851">A computational approach to edge detection</a> by John Canny &#40;1986&#41;</p>
</li>
<li><p><a href="https://hastie.su.domains/Papers/spc_jcgs.pdf">The Sparse PCA paper</a> by Zou, Hastie, Tibshirani &#40;2006&#41;.</p>
</li>
<li><p><a href="https://arxiv.org/abs/math/0403022">The BBP transition</a> by Baik, Ben Arous, and Péché &#40;2004&#41;. Probably the most important paper in random matrix theory. </p>
</li>
<li><p><a href="https://proceedings.neurips.cc/paper/2001/file/801272ee79cfde7fa5960571fee36b9b-Paper.pdf">On spectral clustering</a> by Ng, Jordan and Weiss &#40;2001&#41;.</p>
</li>
<li><p><a href="https://www.jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf">Hyvärinen&#39;s Score Matching paper</a> in 2005. </p>
</li>
<li><p><a href="https://dl.acm.org/doi/pdf/10.1145/2184319.2184343">Exact Matrix Completion via Convex Optimization</a> by Candes and Recht &#40;2009&#41;. Matrix completion via optimization.</p>
</li>
<li><p><a href="https://arxiv.org/pdf/0901.3150.pdf">Matrix completion from a few entries</a> by Keshavan, Montanari and Oh &#40;2009&#41;. Matrix completion from SVD tresholding is &#40;was ?&#41; the go-to method for sparse matrix completion. </p>
</li>
<li><p><a href="https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf">Adaptive mixtures of experts</a> by Jacobs et al. Introduces the famous MoE method that was popularized recently by Mistral. </p>
</li>
<li><p><a href="https://arxiv.org/pdf/cond-mat/9910332">Emergence of scaling in random networks</a>, the original paper by Barabasi and Albert &#40;1999&#41;</p>
</li>
<li><p><a href="https://www.pnas.org/doi/full/10.1073/pnas.1802705116?doi&#61;10.1073/pnas.1802705116">Error in high-dimensional GLMs</a> by Barbier et al. &#40;2018&#41;</p>
</li>
<li><p><a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.108.188701">Spectral algorithms for clustering</a> by Nadakuditi and Newman, .from an RMT perspective</p>
</li>
<li><p><a href="https://www.pnas.org/doi/pdf/10.1073/pnas.1312486110">Spectral redemption in clustering sparse networks</a> by Krzkala et al. &#40;2013&#41;: classical versions of spectral clustering are failing for sparse graphs, but the authors show that a simple modification of the Laplacian matrix can lead to a successful clustering.</p>
</li>
<li><p><a href="https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-33/issue-3/On-Estimation-of-a-Probability-Density-Function-and-Mode/10.1214/aoms/1177704472.full">On Estimation of a Probability Density Function and Mode</a>, the famous kernel density estimation paper by Parzen &#40;1962&#41;</p>
</li>
<li><p><a href="https://arxiv.org/pdf/cond-mat/0412004.pdf">Power laws, Pareto distributions and Zipf’s law</a>, the survey by Newman on heavy-tails</p>
</li>
<li><p><a href="http://www.econ.uiuc.edu/~econ536/Papers/hill75.pdf">Hill’s estimator</a>, a simple way of estimating tails of distributions when they are heavy. </p>
</li>
<li><p><a href="https://www.robots.ox.ac.uk/~az/lectures/ml/tenenbaum-isomap-Science2000.pdf">ISOMAP, nonlinear dimensionality reduction</a> for manifold learning. </p>
</li>
<li><p><a href="https://jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf">t-SNE</a>, the paper introducing the t-SNE dimension reduction technique, by van der Maaten and Hinton &#40;2008&#41;</p>
</li>
<li><p><a href="https://www.stat.cmu.edu/~ryantibs/papers/bestsubset.pdf">Best subset or Lasso</a>, by Hastie, Tibshirani and Friedman &#40;2017&#41;</p>
</li>
<li><p><a href="https://tlakoba.w3.uvm.edu/AppliedUGMath/auxpaper_Reinsch_1967.pdf">Smoothing by spline functions</a>, one of the seminal papers on spline smoothing, by Reinsh &#40;1967&#41;</p>
</li>
<li><p><a href="https://sites.stat.washington.edu/courses/stat527/s14/readings/Silverman_Annals_1984.pdf">Spline smoothing is almost kernel smoothing</a>, a striking paper by Silverman &#40;1984&#41;, and <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber&#61;8611353">its generalization</a> by Ong, Milanfar and Getreuer &#40;2019&#41;. Global optimization problems &#40;such as interpolation&#41; can be approximated by local operations &#40;kernel smoothing&#41;.</p>
</li>
<li><p><a href="https://efron.ckirby.su.domains/papers/2011TweediesFormula.pdf">Tweediess formula and selection bias</a>, a landmark paper by Bradley Efron. Tweedie&#39;s formula is key to many techniques in statistics, including diffusion-based generative models.  </p>
</li>
<li><p><a href="https://leon.bottou.org/publications/pdf/nc-1992.pdf">Local Learning algorithms</a> by Bottou and Vapnik &#40;1992&#41;. The paper that introduced the concept of local learning algorithms.</p>
</li>
<li><p><a href="https://www2.math.uu.se/~thulin/mm/breiman.pdf">The two cultures of Stats</a></p>
</li>
<li><p><a href="https://www2.stat.duke.edu/courses/Spring09/sta122/Readings/EfronWhyEveryone.pdf">Why isn&#39;t everyone a Bayesian?</a> by Efron &#40;2013&#41;. It seems to me, as of 2025, that almost no one remains a Bayesian – except academics living in the bubble of Bayesian statistics, of course. </p>
</li>
</ul>
<h2 id="modern_deep_learning_and_ai"><a href="#modern_deep_learning_and_ai" class="header-anchor">Modern Deep Learning and AI</a></h2>
<h3 id="architectures_and_fundamentals"><a href="#architectures_and_fundamentals" class="header-anchor">Architectures and fundamentals</a></h3>
<ul>
<li><p><a href="https://arxiv.org/abs/1412.6980">The ADAM optimizer</a> by Kingma and Ba &#40;2014&#41;.</p>
</li>
<li><p><a href="https://arxiv.org/abs/1502.03167">The BatchNorm paper</a> by Ioffe and Szegedy &#40;2015&#41;.</p>
</li>
<li><p><a href="https://arxiv.org/pdf/1607.06450">The LayerNorm paper</a> by Ba et al. &#40;2016&#41;.</p>
</li>
<li><p><a href="https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf">The Dropout paper</a> by Srivastava et al. &#40;2014&#41;.</p>
</li>
<li><p><a href="https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">The AlexNet paper</a> by Krizhevsky, Sutskever, and Hinton &#40;2012&#41;.</p>
</li>
<li><p><a href="https://scholar.google.com/citations?view_op&#61;view_citation&amp;hl&#61;en&amp;user&#61;0XfFckgAAAAJ&amp;citation_for_view&#61;0XfFckgAAAAJ:L8Ckcad2t8MC">Density estimation by dual ascent of the log-likelihood</a> by Tabak and Vanden-Eijnden &#40;2010&#41;, first definition of coupling layers for normalizing flows. </p>
</li>
<li><p><a href="https://proceedings.mlr.press/v37/rezende15.pdf">Normalizing flows</a> by Rezende and Mohamed &#40;2015&#41;. They&#39;re not so popular now, but the paper is really a gem. </p>
</li>
<li><p><a href="https://arxiv.org/pdf/1812.09902.pdf">Invariant and equivariant graph networks</a> by Maron et al. &#40;2019&#41;. They compute the dimension of invariant and equivariant linear layers and study GNN expressivity. </p>
</li>
<li><p><a href="https://arxiv.org/abs/1503.03585">The original paper introducing generative diffusion models</a>, by Sohl-Dickstein et al &#40;2015&#41;</p>
</li>
<li><p><a href="https://arxiv.org/abs/2011.13456">The second paper of diffusions</a> by Song et al &#40;2020&#41;, where they notably detailed the SDE formulation. </p>
</li>
<li><p><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf">The Stable Diffusion paper</a> by Rombach et al &#40;2021&#41;</p>
</li>
<li><p><a href="https://arxiv.org/abs/1806.07366">The Neural ODE paper</a> by Chen et al. &#40;2018&#41;</p>
</li>
<li><p><a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a>, 2017. No comment.</p>
</li>
<li><p><a href="https://arxiv.org/abs/2210.02747">Flow matching</a> by Lipman et al, 2022, the most elegant generalization of diffusion models. Flow matching models are now SOTA and it is clear that diffusions will, at some point, disappear. </p>
</li>
<li><p><a href="https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpa.21975">The data-driven Schrödinger bridge</a> by Pavon, Tabak and Trigila &#40;2021&#41;</p>
</li>
<li><p><a href="https://proceedings.mlr.press/v70/arjovsky17a/arjovsky17a.pdf">The Wasserstein GAN paper</a> by Arzovsky, Chintala and Bottou &#40;2017&#41;</p>
</li>
<li><p><a href="https://arxiv.org/abs/2201.09792">The Convmixer paper</a>: fitting a big convolutional network in a tweet. </p>
</li>
<li><p><a href="https://arxiv.org/abs/1506.02640">YOLO</a>, now at its 11th version and still improving&#33;</p>
</li>
<li><p><a href="https://arxiv.org/pdf/1912.01412.pdf">Deep learning for symbolic mathematics</a> by Lample and Charton &#40;2019&#41;</p>
</li>
<li><p><a href="https://arxiv.org/abs/2010.11929">An image is worth 16x16 words</a>, the original Vision Transformer paper by Dosovitskiy et al. &#40;2020&#41;. The paper that started the revolution of transformers in computer vision.</p>
</li>
<li><p><a href="https://arxiv.org/pdf/2304.07193">The Dinov2</a> paper from FAIR explains how they scaled a fully self-supervised feature extractor. That is, they learned the feature extractor &#40;a ViT&#41; using a dataset containing ONLY images: no labels, no text. The teacher/student setting is set up in a particularly smart way. </p>
</li>
<li><p><a href="https://arxiv.org/pdf/2107.06278">Per-Pixel Classification is Not All You Need for Semantic Segmentation</a> : the paper that convinced me that segmentation is actually <em>really</em> hard. </p>
</li>
<li><p><a href="https://arxiv.org/pdf/2304.02643">Segment Anything</a>, the original paper on segmentation by Kirillov et al. &#40;2023&#41; which really pushed the field forward.</p>
</li>
</ul>
<h3 id="theoretical_aspects"><a href="#theoretical_aspects" class="header-anchor">Theoretical aspects</a></h3>
<ul>
<li><p><a href="https://link.springer.com/article/10.1007/BF02551274">Universal approximation theorem</a> by Cybenko &#40;1989&#41;. Roughly speaking, classes of neural networks are dense. </p>
</li>
<li><p><a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf">Language models are few-shot learners</a> on LLM scaling laws.</p>
</li>
<li><p><a href="https://www.jmlr.org/papers/volume22/20-410/20-410.pdf">Implicit regularization in deep networks</a> by Martin and Mahonney &#40;2021&#41;. On the training dynamics of the hessian spectrum of DNNs. </p>
</li>
<li><p><a href="https://arxiv.org/abs/2103.00065">Edge of Stability paper</a> by Cohen et al. </p>
</li>
<li><p><a href="https://arxiv.org/abs/2310.18988">A U-turn on double descent</a> by Curth et al. </p>
</li>
<li><p><a href="https://arxiv.org/abs/1806.07572">The NTK paper</a> by Jacot, Gabriel and Hongler &#40;2018&#41;.</p>
</li>
</ul>
<h3 id="clever_tricks_and_techniques"><a href="#clever_tricks_and_techniques" class="header-anchor">Clever tricks and techniques</a></h3>
<ul>
<li><p><a href="https://arxiv.org/pdf/1912.08193">Image Segmentation as rendering</a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2309.16588">ViTs need Registers</a>, by T. Darcet et al. A strikingly simple observation : ViT store some internal informations inside the tokens, because it needs to store them somewhere. Adding a small &quot;memory register&quot; &#40;ie, additional tokens&#41; solves it. A very nice scientific paper.  </p>
</li>
<li><p><a href="https://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html">Chain-of-Thought</a>, a landmark technique for having better at inference time. Partly responsible for the huge gap in reasonning quality of LLMs between 23 and 24.</p>
</li>
<li><p><a href="https://arxiv.org/abs/2104.09864">Rotary Positional Encoding</a>: previously, positional encoding did not retain <em>relative position</em> information. By working in the complex plane with rotations, this is no longer a problem. This solution is now practically implemented everywhere.  </p>
</li>
<li><p><a href="https://arxiv.org/pdf/2411.04282">LatRon</a>, the very recent Latent Reasonning paper : instead of asking a LLM to generate an answer <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span> to a question <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">x</span></span></span></span>, you can fine-tune it to generate &quot;the&quot; question <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span></span></span></span> which, given to the LLM, maximizes the probability of answer <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span> –- and it&#39;s often <em>not</em> the raw question <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">x</span></span></span></span> &#33; A very clever and simple trick. </p>
</li>
</ul>
<!-- 
  Page footer is in the sidebar. 
 --></div><!-- CONTENT ENDS HERE -->
    </div>  <!-- div: content container -->
    
        



    
    
        


    
  </body>
</html>
